{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic回归的一般过程\n",
    "  \n",
    "- (1) 收集数据: 采用任意方法收集数据。\n",
    "- (2) 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据\n",
    "格式则最佳。\n",
    "- (3) 分析数据: 采用任意方法对数据进行分析。\n",
    "- (4) 训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。\n",
    "- (5) 测试算法: 一旦训练步骤完成，分类将会很快。\n",
    "- (6) 使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值;接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别;在这之后，我们就可以在输出的类别上做一些其他分析工作\n",
    "\n",
    "#### Logistic回归\n",
    "  \n",
    "- 优点:计算代价不高，易于理解和实现。\n",
    "- 缺点:容易欠拟合，分类精度可能不高。\n",
    "- 适用数据类型:数值型和标称型数据。\n",
    "\n",
    "#### 梯度上升法的伪代码如下:\n",
    "每个回归系数初始化为1 \n",
    "\n",
    "重复R次:\n",
    "- 计算整个数据集的梯度\n",
    "- 使用alpha × gradient更新回归系数的向量 \n",
    "- 返回回归系数\n",
    "\n",
    "缺点：\n",
    "- 适用于样本数量少和特征值少的情况\n",
    "- 每次更新回归系数都要遍历整个数据集，计算复杂度高；若样本数据大特征值多，则严重消耗内存资源。\n",
    "\n",
    "#### 随机梯度上升算法\n",
    "一次仅用一个样本点来更新回归系数\n",
    "\n",
    "随机梯度上升算法伪代码：\n",
    "\n",
    "- 所有回归系数初始化为1 \n",
    "- 对数据集中每个样本计算该样本的梯度\n",
    "    - 使用alpha × gradient更新回归系数值 \n",
    "- 返回回归系数值\n",
    "\n",
    "#### 改进随机梯度上升算法\n",
    "上述随机梯度上升算法效果不理想\n",
    "\n",
    "改进：\n",
    "- 步长alpha动态减小，而不是使用固定值\n",
    "- 从样本集中随机选择样本\n",
    "- 增加迭代次数\n",
    "\n",
    "#### 示例:使用Logistic回归估计马疝病的死亡率\n",
    "  \n",
    "- (1) 收集数据:给定数据文件。\n",
    "- (2) 准备数据:用Python解析文本文件并填充缺失值。\n",
    "- (3) 分析数据:可视化并观察数据。\n",
    "- (4) 训练算法:使用优化算法，找到最佳的系数。\n",
    "- (5) 测试算法:为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数。\n",
    "- (6) 使用算法:实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以做为留给读者的一道习题。\n",
    "\n",
    "#### 准备数据：处理数据中的缺失值\n",
    "下面给出了一些可选的做法:\n",
    "- 使用可用特征的均值来填补缺失值;\n",
    "- 使用特殊值来填补缺失值，如-1;\n",
    "- 忽略有缺失值的样本;\n",
    "- 使用相似样本的均值添补缺失值;\n",
    "- 使用另外的机器学习算法预测缺失值。\n",
    "\n",
    "如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换。采用Logistic回归进行分类时这种做法是合理的，而如果采用类似kNN的方法就可能不太可行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T05:31:22.557205Z",
     "start_time": "2019-07-10T05:31:21.797913Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    加载数据集\n",
    "    @返回值 dataMat:数据集; labelMat:分类标签\n",
    "    \"\"\"\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open('testSet.txt')\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split()\n",
    "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(int(lineArr[2]))\n",
    "    return dataMat,labelMat\n",
    "\n",
    "def sigmoid(inX):\n",
    "    \"\"\"\n",
    "    sigmoid函数\n",
    "    @params: inX:特征值与对应回归系数乘积和\n",
    "    \"\"\"\n",
    "    return 1.0/(1+exp(-inX))\n",
    "\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    \"\"\"\n",
    "    梯度上升算法计算回归系数\n",
    "    @params: dataMatIn:输入数据，整个训练集；classLabels：每个训练样本所对应的标签\n",
    "    @返回值: 回归系数\n",
    "    \"\"\"\n",
    "    dataMatrix = mat(dataMatIn)             \n",
    "    labelMat = mat(classLabels).transpose() \n",
    "    m,n = shape(dataMatrix)\n",
    "    alpha = 0.001   # 步长\n",
    "    maxCycles = 500 # 迭代次数\n",
    "    weights = ones((n,1))   # 初始化回归系数\n",
    "    for k in range(maxCycles):              \n",
    "        h = sigmoid(dataMatrix*weights)   # 计算sigmoid函数值作为预测值  \n",
    "        error = (labelMat - h)           # 计算真实值与预测值之间的误差\n",
    "        weights = weights + alpha * dataMatrix.transpose()* error # 更新回归系数\n",
    "    return weights\n",
    "\n",
    "def plotBestFit(weights):\n",
    "    \"\"\"\n",
    "    画出决策边界\n",
    "    @params: weights: 利用梯度上升法计算出的回归系数；任何一个样本实例通过与该回归系数做乘积即可得到预测分类结果\n",
    "    \"\"\"\n",
    "    dataMat,labelMat=loadDataSet()\n",
    "    dataArr = array(dataMat)\n",
    "    n = shape(dataArr)[0] \n",
    "    xcord1 = []; ycord1 = []\n",
    "    xcord2 = []; ycord2 = []\n",
    "    for i in range(n):\n",
    "        if int(labelMat[i])== 1:\n",
    "            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
    "        else:\n",
    "            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    # 画出原始数据的散点图\n",
    "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
    "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
    "    \n",
    "    # 画出根据回归系数算出的拟合回归直线\n",
    "    x = arange(-3.0, 3.0, 0.1)\n",
    "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel('X1'); plt.ylabel('X2');\n",
    "    plt.show()\n",
    "\n",
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "    \"\"\"\n",
    "    一次仅用一个样本点来更新回归系数\n",
    "    随机梯度上升算法:dataMatIn:输入数据，整个训练集；classLabels：每个训练样本所对应的标签\n",
    "    @返回值: 回归系数\n",
    "    \"\"\"\n",
    "    m,n = shape(dataMatrix)\n",
    "    alpha = 0.01   # 步长\n",
    "    weights = ones(n)   # 初始化回归系数\n",
    "    for i in range(m):\n",
    "        h = sigmoid(sum(dataMatrix[i]*weights))  # 计算sigmoid函数，计算该样本的预测值\n",
    "        error = classLabels[i] - h     # 计算真实值与预测值的误差\n",
    "        weights = weights + alpha * error * dataMatrix[i]   # 更新回归系数\n",
    "    return weights\n",
    "\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    \"\"\"\n",
    "    改进的随机梯度上升算法\n",
    "    @params: dataMatIn:输入数据，整个训练集；classLabels：每个训练样本所对应的标签; numIter：迭代次数，默认150次\n",
    "    @返回值; 回归系数\n",
    "    \"\"\"\n",
    "    m,n = shape(dataMatrix)\n",
    "    weights = ones(n)   # 初始化回归系数\n",
    "    for j in range(numIter):\n",
    "        dataIndex = range(m)\n",
    "        for i in range(m):\n",
    "            alpha = 4/(1.0+j+i)+0.0001    #alpha 步长动态减少\n",
    "            randIndex = int(random.uniform(0,len(dataIndex))) #随机选择一个样本计算回归系数\n",
    "            h = sigmoid(sum(dataMatrix[randIndex]*weights))  # 单样本计算sigmoid，该样本的预测分类值\n",
    "            error = classLabels[randIndex] - h    # 计算真实值与预测值之间的误差\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            del(dataIndex[randIndex])    # 从数据集中删除随机选择的那个样本\n",
    "    return weights\n",
    "\n",
    "def classifyVector(inX, weights):\n",
    "    \"\"\"\n",
    "    分类\n",
    "    @params: inX:特征向量；weights:回归系数\n",
    "    @返回值:Sigmoid值大于0.5函数返回1，否则返回0\n",
    "    \"\"\"\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5: \n",
    "        return 1.0\n",
    "    else: \n",
    "        return 0.0\n",
    "\n",
    "def colicTest():\n",
    "    \"\"\"\n",
    "    利用logistic和随机梯度上升算法 实例化 计算病马的死亡率\n",
    "    @返回值;返回在测试集上预测的错误率\n",
    "    \"\"\"\n",
    "    frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')\n",
    "    trainingSet = []; trainingLabels = []   # 训练集，训练集对应的标签\n",
    "    for line in frTrain.readlines():\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr =[]\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    \n",
    "    # 使用改进的随机梯度上升方法 stocGradAscent1(dataMatrix, classLabels, numIter=150)\n",
    "    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)\n",
    "    errorCount = 0; numTestVec = 0.0\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1.0\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr =[]\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    errorRate = (float(errorCount)/numTestVec)\n",
    "    print(f\"在测试集上预测的错误率为:{errorRate}\")\n",
    "    return errorRate\n",
    "\n",
    "def multiTest():\n",
    "    \"\"\"\n",
    "    计算在测试集上的平均错误率\n",
    "    \"\"\"\n",
    "    numTests = 10; errorSum=0.0\n",
    "    for k in range(numTests):\n",
    "        errorSum += colicTest()\n",
    "    print(f\"10次迭代，平均错误率为: {errorSum/float(numTests)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Maching Learning",
   "language": "python",
   "name": "maching-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
