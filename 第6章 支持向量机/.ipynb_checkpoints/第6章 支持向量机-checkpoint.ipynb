{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM有很多实现，但这里只关注其中最流行的一种实现，即序列最小优化(Sequential Minimal Optimization)SMO算法。\n",
    "\n",
    "#### 基于最大间隔分割数据\n",
    "支持向量机\n",
    "  \n",
    "- 优点:泛化错误率低，计算开销不大，结果易解释。\n",
    "- 缺点:对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。\n",
    "- 适用数据类型:数值型和标称型数据。\n",
    "\n",
    "#### 分隔超平面：分类的决策边界\n",
    "\n",
    "#### 支持向量\n",
    "找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为间隔。我 们希望间隔尽可能地大，这是因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分。类器尽可能健壮。支持向量就是离分隔超平面最近的那些点。最大化支持向量到分隔面的距离。\n",
    "\n",
    "#### SVM应用的一般框架\n",
    "- (1) 收集数据:可以使用任意方法。\n",
    "- (2) 准备数据:需要数值型数据。\n",
    "- (3) 分析数据:有助于可视化分隔超平面。\n",
    "- (4) 训练算法:SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。\n",
    "- (5) 测试算法:十分简单的计算过程就可以实现。\n",
    "- (6) 使用算法:几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from time import sleep\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"\n",
    "    加载数据集\n",
    "    :param fileName:数据集文件路径\n",
    "    :return dataMat: 数据集矩阵\n",
    "    :return labelMat:类标签\n",
    "    \"\"\"\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(float(lineArr[2]))\n",
    "    return dataMat,labelMat\n",
    "\n",
    "\n",
    "def selectJrand(i,m):\n",
    "    \"\"\"\n",
    "    随机选择J的值\n",
    "    :param i:第一个alpha的下标\n",
    "    :param m: 所有alpha的数目\n",
    "    :return j:一旦i不等于j，函数返回随机选择的值\n",
    "    \"\"\"\n",
    "    j=i \n",
    "    while (j==i):\n",
    "        j = int(random.uniform(0,m))\n",
    "    return j\n",
    "\n",
    "def clipAlpha(aj,H,L):\n",
    "    \"\"\"\n",
    "    调整大于H或小于L的alpha值\n",
    "    \"\"\"\n",
    "    if aj > H: \n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj\n",
    "\n",
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()\n",
    "    b = 0; m,n = shape(dataMatrix)\n",
    "    alphas = mat(zeros((m,1)))\n",
    "    iter = 0\n",
    "    while (iter < maxIter):\n",
    "        alphaPairsChanged = 0\n",
    "        for i in range(m):\n",
    "            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\n",
    "            Ei = fXi - float(labelMat[i])#if checks if an example violates KKT conditions\n",
    "            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n",
    "                j = selectJrand(i,m)\n",
    "                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\n",
    "                Ej = fXj - float(labelMat[j])\n",
    "                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\n",
    "                if (labelMat[i] != labelMat[j]):\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                if L==H: print \"L==H\"; continue\n",
    "                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                if eta >= 0: print \"eta>=0\"; continue\n",
    "                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n",
    "                alphas[j] = clipAlpha(alphas[j],H,L)\n",
    "                if (abs(alphas[j] - alphaJold) < 0.00001): print \"j not moving enough\"; continue\n",
    "                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#update i by the same amount as j\n",
    "                                                                        #the update is in the oppostie direction\n",
    "                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n",
    "                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                if (0 < alphas[i]) and (C > alphas[i]): b = b1\n",
    "                elif (0 < alphas[j]) and (C > alphas[j]): b = b2\n",
    "                else: b = (b1 + b2)/2.0\n",
    "                alphaPairsChanged += 1\n",
    "                print \"iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)\n",
    "        if (alphaPairsChanged == 0): iter += 1\n",
    "        else: iter = 0\n",
    "        print \"iteration number: %d\" % iter\n",
    "    return b,alphas\n",
    "\n",
    "def kernelTrans(X, A, kTup): #calc the kernel or transform data to a higher dimensional space\n",
    "    m,n = shape(X)\n",
    "    K = mat(zeros((m,1)))\n",
    "    if kTup[0]=='lin': K = X * A.T   #linear kernel\n",
    "    elif kTup[0]=='rbf':\n",
    "        for j in range(m):\n",
    "            deltaRow = X[j,:] - A\n",
    "            K[j] = deltaRow*deltaRow.T\n",
    "        K = exp(K/(-1*kTup[1]**2)) #divide in NumPy is element-wise not matrix like Matlab\n",
    "    else: raise NameError('Houston We Have a Problem -- \\\n",
    "    That Kernel is not recognized')\n",
    "    return K\n",
    "\n",
    "class optStruct:\n",
    "    def __init__(self,dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters \n",
    "        self.X = dataMatIn\n",
    "        self.labelMat = classLabels\n",
    "        self.C = C\n",
    "        self.tol = toler\n",
    "        self.m = shape(dataMatIn)[0]\n",
    "        self.alphas = mat(zeros((self.m,1)))\n",
    "        self.b = 0\n",
    "        self.eCache = mat(zeros((self.m,2))) #first column is valid flag\n",
    "        self.K = mat(zeros((self.m,self.m)))\n",
    "        for i in range(self.m):\n",
    "            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\n",
    "        \n",
    "def calcEk(oS, k):\n",
    "    fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n",
    "    Ek = fXk - float(oS.labelMat[k])\n",
    "    return Ek\n",
    "        \n",
    "def selectJ(i, oS, Ei):         #this is the second choice -heurstic, and calcs Ej\n",
    "    maxK = -1; maxDeltaE = 0; Ej = 0\n",
    "    oS.eCache[i] = [1,Ei]  #set valid #choose the alpha that gives the maximum delta E\n",
    "    validEcacheList = nonzero(oS.eCache[:,0].A)[0]\n",
    "    if (len(validEcacheList)) > 1:\n",
    "        for k in validEcacheList:   #loop through valid Ecache values and find the one that maximizes delta E\n",
    "            if k == i: continue #don't calc for i, waste of time\n",
    "            Ek = calcEk(oS, k)\n",
    "            deltaE = abs(Ei - Ek)\n",
    "            if (deltaE > maxDeltaE):\n",
    "                maxK = k; maxDeltaE = deltaE; Ej = Ek\n",
    "        return maxK, Ej\n",
    "    else:   #in this case (first time around) we don't have any valid eCache values\n",
    "        j = selectJrand(i, oS.m)\n",
    "        Ej = calcEk(oS, j)\n",
    "    return j, Ej\n",
    "\n",
    "def updateEk(oS, k):#after any alpha has changed update the new value in the cache\n",
    "    Ek = calcEk(oS, k)\n",
    "    oS.eCache[k] = [1,Ek]\n",
    "        \n",
    "def innerL(i, oS):\n",
    "    Ei = calcEk(oS, i)\n",
    "    if ((oS.labelMat[i]*Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i]*Ei > oS.tol) and (oS.alphas[i] > 0)):\n",
    "        j,Ej = selectJ(i, oS, Ei) #this has been changed from selectJrand\n",
    "        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n",
    "        if (oS.labelMat[i] != oS.labelMat[j]):\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L==H: print \"L==H\"; return 0\n",
    "        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] #changed for kernel\n",
    "        if eta >= 0: print \"eta>=0\"; return 0\n",
    "        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n",
    "        updateEk(oS, j) #added this for the Ecache\n",
    "        if (abs(oS.alphas[j] - alphaJold) < 0.00001): print \"j not moving enough\"; return 0\n",
    "        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j\n",
    "        updateEk(oS, i) #added this for the Ecache                    #the update is in the oppostie direction\n",
    "        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n",
    "        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n",
    "        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n",
    "        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n",
    "        else: oS.b = (b1 + b2)/2.0\n",
    "        return 1\n",
    "    else: return 0\n",
    "\n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=('lin', 0)):    #full Platt SMO\n",
    "    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)\n",
    "    iter = 0\n",
    "    entireSet = True; alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:   #go over all\n",
    "            for i in range(oS.m):        \n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print \"fullSet, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)\n",
    "            iter += 1\n",
    "        else:#go over non-bound (railed) alphas\n",
    "            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print \"non-bound, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)\n",
    "            iter += 1\n",
    "        if entireSet: entireSet = False #toggle entire set loop\n",
    "        elif (alphaPairsChanged == 0): entireSet = True  \n",
    "        print \"iteration number: %d\" % iter\n",
    "    return oS.b,oS.alphas\n",
    "\n",
    "def calcWs(alphas,dataArr,classLabels):\n",
    "    X = mat(dataArr); labelMat = mat(classLabels).transpose()\n",
    "    m,n = shape(X)\n",
    "    w = zeros((n,1))\n",
    "    for i in range(m):\n",
    "        w += multiply(alphas[i]*labelMat[i],X[i,:].T)\n",
    "    return w\n",
    "\n",
    "def testRbf(k1=1.3):\n",
    "    dataArr,labelArr = loadDataSet('testSetRBF.txt')\n",
    "    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1)) #C=200 important\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    svInd=nonzero(alphas.A>0)[0]\n",
    "    sVs=datMat[svInd] #get matrix of only support vectors\n",
    "    labelSV = labelMat[svInd];\n",
    "    print \"there are %d Support Vectors\" % shape(sVs)[0]\n",
    "    m,n = shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1\n",
    "    print \"the training error rate is: %f\" % (float(errorCount)/m)\n",
    "    dataArr,labelArr = loadDataSet('testSetRBF2.txt')\n",
    "    errorCount = 0\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    m,n = shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1    \n",
    "    print \"the test error rate is: %f\" % (float(errorCount)/m)    \n",
    "    \n",
    "def img2vector(filename):\n",
    "    returnVect = zeros((1,1024))\n",
    "    fr = open(filename)\n",
    "    for i in range(32):\n",
    "        lineStr = fr.readline()\n",
    "        for j in range(32):\n",
    "            returnVect[0,32*i+j] = int(lineStr[j])\n",
    "    return returnVect\n",
    "\n",
    "def loadImages(dirName):\n",
    "    from os import listdir\n",
    "    hwLabels = []\n",
    "    trainingFileList = listdir(dirName)           #load the training set\n",
    "    m = len(trainingFileList)\n",
    "    trainingMat = zeros((m,1024))\n",
    "    for i in range(m):\n",
    "        fileNameStr = trainingFileList[i]\n",
    "        fileStr = fileNameStr.split('.')[0]     #take off .txt\n",
    "        classNumStr = int(fileStr.split('_')[0])\n",
    "        if classNumStr == 9: hwLabels.append(-1)\n",
    "        else: hwLabels.append(1)\n",
    "        trainingMat[i,:] = img2vector('%s/%s' % (dirName, fileNameStr))\n",
    "    return trainingMat, hwLabels    \n",
    "\n",
    "def testDigits(kTup=('rbf', 10)):\n",
    "    dataArr,labelArr = loadImages('trainingDigits')\n",
    "    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    svInd=nonzero(alphas.A>0)[0]\n",
    "    sVs=datMat[svInd] \n",
    "    labelSV = labelMat[svInd];\n",
    "    print \"there are %d Support Vectors\" % shape(sVs)[0]\n",
    "    m,n = shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1\n",
    "    print \"the training error rate is: %f\" % (float(errorCount)/m)\n",
    "    dataArr,labelArr = loadImages('testDigits')\n",
    "    errorCount = 0\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    m,n = shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1    \n",
    "    print \"the test error rate is: %f\" % (float(errorCount)/m) \n",
    "\n",
    "\n",
    "'''#######********************************\n",
    "Non-Kernel VErsions below\n",
    "'''#######********************************\n",
    "\n",
    "class optStructK:\n",
    "    def __init__(self,dataMatIn, classLabels, C, toler):  # Initialize the structure with the parameters \n",
    "        self.X = dataMatIn\n",
    "        self.labelMat = classLabels\n",
    "        self.C = C\n",
    "        self.tol = toler\n",
    "        self.m = shape(dataMatIn)[0]\n",
    "        self.alphas = mat(zeros((self.m,1)))\n",
    "        self.b = 0\n",
    "        self.eCache = mat(zeros((self.m,2))) #first column is valid flag\n",
    "        \n",
    "def calcEkK(oS, k):\n",
    "    fXk = float(multiply(oS.alphas,oS.labelMat).T*(oS.X*oS.X[k,:].T)) + oS.b\n",
    "    Ek = fXk - float(oS.labelMat[k])\n",
    "    return Ek\n",
    "        \n",
    "def selectJK(i, oS, Ei):         #this is the second choice -heurstic, and calcs Ej\n",
    "    maxK = -1; maxDeltaE = 0; Ej = 0\n",
    "    oS.eCache[i] = [1,Ei]  #set valid #choose the alpha that gives the maximum delta E\n",
    "    validEcacheList = nonzero(oS.eCache[:,0].A)[0]\n",
    "    if (len(validEcacheList)) > 1:\n",
    "        for k in validEcacheList:   #loop through valid Ecache values and find the one that maximizes delta E\n",
    "            if k == i: continue #don't calc for i, waste of time\n",
    "            Ek = calcEk(oS, k)\n",
    "            deltaE = abs(Ei - Ek)\n",
    "            if (deltaE > maxDeltaE):\n",
    "                maxK = k; maxDeltaE = deltaE; Ej = Ek\n",
    "        return maxK, Ej\n",
    "    else:   #in this case (first time around) we don't have any valid eCache values\n",
    "        j = selectJrand(i, oS.m)\n",
    "        Ej = calcEk(oS, j)\n",
    "    return j, Ej\n",
    "\n",
    "def updateEkK(oS, k):#after any alpha has changed update the new value in the cache\n",
    "    Ek = calcEk(oS, k)\n",
    "    oS.eCache[k] = [1,Ek]\n",
    "        \n",
    "def innerLK(i, oS):\n",
    "    Ei = calcEk(oS, i)\n",
    "    if ((oS.labelMat[i]*Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i]*Ei > oS.tol) and (oS.alphas[i] > 0)):\n",
    "        j,Ej = selectJ(i, oS, Ei) #this has been changed from selectJrand\n",
    "        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n",
    "        if (oS.labelMat[i] != oS.labelMat[j]):\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L==H: print \"L==H\"; return 0\n",
    "        eta = 2.0 * oS.X[i,:]*oS.X[j,:].T - oS.X[i,:]*oS.X[i,:].T - oS.X[j,:]*oS.X[j,:].T\n",
    "        if eta >= 0: print \"eta>=0\"; return 0\n",
    "        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n",
    "        updateEk(oS, j) #added this for the Ecache\n",
    "        if (abs(oS.alphas[j] - alphaJold) < 0.00001): print \"j not moving enough\"; return 0\n",
    "        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j\n",
    "        updateEk(oS, i) #added this for the Ecache                    #the update is in the oppostie direction\n",
    "        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[i,:]*oS.X[j,:].T\n",
    "        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[j,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[j,:]*oS.X[j,:].T\n",
    "        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n",
    "        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n",
    "        else: oS.b = (b1 + b2)/2.0\n",
    "        return 1\n",
    "    else: return 0\n",
    "\n",
    "def smoPK(dataMatIn, classLabels, C, toler, maxIter):    #full Platt SMO\n",
    "    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler)\n",
    "    iter = 0\n",
    "    entireSet = True; alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:   #go over all\n",
    "            for i in range(oS.m):        \n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print \"fullSet, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)\n",
    "            iter += 1\n",
    "        else:#go over non-bound (railed) alphas\n",
    "            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print \"non-bound, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)\n",
    "            iter += 1\n",
    "        if entireSet: entireSet = False #toggle entire set loop\n",
    "        elif (alphaPairsChanged == 0): entireSet = True  \n",
    "        print \"iteration number: %d\" % iter\n",
    "    return oS.b,oS.alphas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Maching Learning",
   "language": "python",
   "name": "maching-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
